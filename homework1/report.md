# Отчет по учебной практике "Основы глубокого обучения и искусственного интеллекта"

## Задание 1: Создание и манипуляции с тензорами

### Результаты выполнения:

#### 1.1 Создание тензоров
- Случайный тензор 3x4:
  ```
  [[0.8019, 0.7794, 0.0714, 0.7040],
   [0.9453, 0.0827, 0.2074, 0.8058],
   [0.5452, 0.3082, 0.5429, 0.8193]]
  ```
- Тензор нулей 2x3x4: успешно создан
- Тензор единиц 5x5: успешно создан
- Тензор 4x4 с числами 0-15:
  ```
  [[ 0,  1,  2,  3],
   [ 4,  5,  6,  7],
   [ 8,  9,10,11],
   [12,13,14,15]]
  ```

#### 1.2 Операции с тензорами
- Транспонирование матрицы A выполнено успешно
- Матричное умножение A и B:
  ```
  [[ 42,  48,  54],
   [114,136,158],
   [186,224,262]]
  ```
- Поэлементное умножение выполнено
- Сумма всех элементов A: 66.0

#### 1.3 Индексация и срезы
- Успешно извлечены: первая строка, последний столбец, подматрица 2x2, элементы с четными индексами

#### 1.4 Работа с формами
- Тензор из 24 элементов успешно преобразован в формы:
  - 2x12, 3x8, 4x6
  - 2x3x4, 2x2x2x3
- Сумма элементов во всех формах: 276 (корректно)

## Задание 2: Автоматическое дифференцирование

### Результаты выполнения:

#### 2.1 Простые вычисления с градиентами
- Функция f(x,y,z) = x² + y² + z² + 2xyz при x=1, y=2, z=3:
  - Значение: 26.0
  - Градиенты:
    - autograd: (14.0, 10.0, 10.0)
    - Аналитические: (14.0, 10.0, 10.0) - результаты совпадают

#### 2.2 Градиент функции MSE
- Линейная модель y_pred = 0.5*x + 1.0
- MSE loss: 10.375
- Градиенты:
  - dL/dw: -17.5
  - dL/db: -5.5

#### 2.3 Цепное правило
- Функция f(x) = sin(x² + 1) при x=2:
  - Значение: -0.9589
  - Градиенты:
    - через backward(): 1.1346
    - через autograd.grad: 1.1346
    - аналитический: 1.1346 - все методы дали одинаковый результат

## Задание 3: Сравнение производительности CPU vs CUDA

### Результаты тестирования (Tesla T4):

#### Матрица 64x1024x1024:
| Операция               | CPU (мс) | GPU (мс) | Ускорение |
|------------------------|----------|----------|-----------|
| Матричное умножение    | 2157.5   | 178.6    | 12.1x     |
| Поэлементное сложение  | 169.4    | 42.0     | 4.0x      |
| Поэлементное умножение | 162.9    | 35.5     | 4.6x      |
| Транспонирование       | 0.1      | 0.1      | 1.0x      |
| Сумма всех элементов   | 31.5     | 39.6     | 0.8x      |

#### Матрица 128x512x512:
| Операция               | CPU (мс) | GPU (мс) | Ускорение |
|------------------------|----------|----------|-----------|
| Матричное умножение    | 560.0    | 13.2     | 42.3x     |
| Поэлементное сложение  | 82.0     | 1.2      | 69.3x     |
| Поэлементное умножение | 78.4     | 1.2      | 66.0x     |
| Транспонирование       | 0.0      | 0.0      | 1.7x      |
| Сумма всех элементов   | 11.7     | 0.6      | 18.5x     |

#### Матрица 256x256x256:
| Операция               | CPU (мс) | GPU (мс) | Ускорение |
|------------------------|----------|----------|-----------|
| Матричное умножение    | 163.0    | 3.6      | 45.0x     |
| Поэлементное сложение  | 38.5     | 0.6      | 59.6x     |
| Поэлементное умножение | 38.1     | 0.6      | 60.3x     |
| Транспонирование       | 0.0      | 0.0      | 1.8x      |
| Сумма всех элементов   | 5.7      | 0.4      | 16.4x     |

### Анализ результатов:
1. Наибольшее ускорение получили матричные операции и поэлементные операции благодаря параллелизации на GPU
2. Простые операции (сумма, транспонирование) показывают меньшее ускорение из-за накладных расходов
3. Для больших матриц ускорение более значительно (до 69x для сложения)
4. Операции с малым объемом вычислений (транспонирование) могут не получать выигрыша на GPU

## Выводы
Все задания выполнены успешно. Результаты показывают:
- Корректную работу с тензорами различных размерностей
- Правильную реализацию автоматического дифференцирования
- Значительный выигрыш в производительности при использовании GPU для матричных операций
- Аналитические вычисления градиентов совпадают с автоматическими
