# Отчёт по учебной практике "Основы глубокого обучения и искусственного интеллекта"

## Задание 1: Создание и манипуляции с тензорами

### 1.1 Создание тензоров

**Реализация:**
- Создан тензор 3x4 со случайными значениями (0-1) с помощью `torch.rand(3, 4)`
- Создан тензор 2x3x4 с нулями через `torch.zeros(2, 3, 4)`
- Сформирован тензор 5x5 с единицами через `torch.ones(5, 5)`
- Создан тензор 4x4 с числами 0-15 через `torch.arange(16).reshape(4, 4)`

**Вывод программы:**
```
Тензор 3x4:
[[0.8019, 0.7794, 0.0714, 0.7040],
 [0.9453, 0.0827, 0.2074, 0.8058],
 [0.5452, 0.3082, 0.5429, 0.8193]]

Тензор 2x3x4:
[[[0., 0., 0., 0.],
  [0., 0., 0., 0.],
  [0., 0., 0., 0.]],
 [[0., 0., 0., 0.],
  [0., 0., 0., 0.],
  [0., 0., 0., 0.]]]

Тензор 5x5:
[[1., 1., 1., 1., 1.],
 [1., 1., 1., 1., 1.],
 [1., 1., 1., 1., 1.],
 [1., 1., 1., 1., 1.],
 [1., 1., 1., 1., 1.]]

Тензор 4x4:
[[ 0,  1,  2,  3],
 [ 4,  5,  6,  7],
 [ 8,  9, 10, 11],
 [12, 13, 14, 15]]
```

**Вывод:** Все тензоры созданы корректно с заданными параметрами. Использование специализированных функций PyTorch (rand, zeros, ones, arange) обеспечило правильное заполнение тензоров требуемыми значениями.

### 1.2 Операции с тензорами

**Реализация:**
Для матриц A(3x4) и B(4x3) выполнены:
1. Транспонирование A через `A.T`
2. Матричное умножение через `torch.matmul(A, B)`
3. Поэлементное умножение через `A * B.T`
4. Сумма элементов через `A.sum()`

**Вывод программы:**
```
A:
[[ 0.,  1.,  2.,  3.],
 [ 4.,  5.,  6.,  7.],
 [ 8.,  9., 10., 11.]]

B:
[[ 0.,  1.,  2.],
 [ 3.,  4.,  5.],
 [ 6.,  7.,  8.],
 [ 9., 10., 11.]]

Транспонированная A:
[[ 0.,  4.,  8.],
 [ 1.,  5.,  9.],
 [ 2.,  6., 10.],
 [ 3.,  7., 11.]]

Матричное умножение:
[[ 42.,  48.,  54.],
 [114., 136., 158.],
 [186., 224., 262.]]

Поэлементное умножение:
[[  0.,   3.,  12.,  27.],
 [  4.,  20.,  42.,  70.],
 [ 16.,  45.,  80., 121.]]

Сумма элементов A: 66.0
```

**Вывод:** Все операции выполнены корректно. Результаты матричного умножения и поэлементных операций соответствуют математическим ожиданиям. Сумма элементов вычислена верно.

### 1.3 Индексация и срезы

**Реализация:**
Для тензора 5x5x5 выполнены:
1. Извлечение первой строки через `tensor[0, :, :]`
2. Извлечение последнего столбца через `tensor[:, :, -1]`
3. Извлечение подматрицы 2x2 через `tensor[2:4, 2:4, 2]`
4. Извлечение элементов с чётными индексами через `tensor[::2, ::2, ::2]`

**Вывод программы:**
```
Первая строка:
[[0.9903, 0.9256, 0.1188, 0.2233, 0.3229],
 [0.3885, 0.9975, 0.5364, 0.1453, 0.8794],
 [0.5048, 0.2783, 0.9545, 0.2722, 0.8149],
 [0.8505, 0.6811, 0.8216, 0.5948, 0.7947],
 [0.6481, 0.6225, 0.5953, 0.8213, 0.1050]]

Последний столбец:
[[0.3229, 0.8794, 0.8149, 0.7947, 0.1050],
 [0.4077, 0.5508, 0.7826, 0.1351, 0.1101],
 [0.9398, 0.3749, 0.6530, 0.9729, 0.6989],
 [0.8785, 0.3438, 0.4469, 0.8761, 0.7773],
 [0.3856, 0.1931, 0.8151, 0.0717, 0.6469]]

Центральная подматрица:
[[0.9975, 0.5364],
 [0.2783, 0.9545]]

Элементы с четными индексами:
[[[0.9903, 0.1188, 0.3229],
  [0.5048, 0.9545, 0.8149],
  [0.6481, 0.5953, 0.1050]],
 [[0.5833, 0.4415, 0.9398],
  [0.9088, 0.8399, 0.6530],
  [0.6088, 0.8710, 0.6989]],
 [[0.6112, 0.5780, 0.3856],
  [0.2297, 0.5198, 0.8151],
  [0.9053, 0.7453, 0.6469]]]
```

**Вывод:** Операции индексации выполнены корректно. Все запрошенные срезы и подматрицы извлечены верно, что подтверждает правильную работу с многомерными тензорами.

### 1.4 Работа с формами

**Реализация:**
Тензор из 24 элементов преобразован в:
- 2x12, 3x8, 4x6 через `reshape()`
- 2x3x4, 2x2x2x3 через `reshape()`

**Вывод программы:**
```
Форма 2x12:
[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],
 [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]

Форма 3x8:
[[ 0,  1,  2,  3,  4,  5,  6,  7],
 [ 8,  9, 10, 11, 12, 13, 14, 15],
 [16, 17, 18, 19, 20, 21, 22, 23]]

Форма 4x6:
[[ 0,  1,  2,  3,  4,  5],
 [ 6,  7,  8,  9, 10, 11],
 [12, 13, 14, 15, 16, 17],
 [18, 19, 20, 21, 22, 23]]

Форма 2x3x4:
[[[ 0,  1,  2,  3],
  [ 4,  5,  6,  7],
  [ 8,  9, 10, 11]],
 [[12, 13, 14, 15],
  [16, 17, 18, 19],
  [20, 21, 22, 23]]]

Форма 2x2x2x3:
[[[[ 0,  1,  2],
   [ 3,  4,  5]],
  [[ 6,  7,  8],
   [ 9, 10, 11]]],
 [[[12, 13, 14],
   [15, 16, 17]],
  [[18, 19, 20],
   [21, 22, 23]]]]
```

**Вывод:** Преобразование форм тензора выполнено успешно. Сумма элементов (276) сохраняется при всех преобразованиях, что подтверждает корректность операций reshape.

### Итог по заданию 1

Все задачи выполнены успешно:
- Различные типы тензоров созданы корректно
- Матричные операции работают как ожидалось
- Индексация и срезы работают правильно
- Изменение формы тензора не влияет на его данные
- Тесты пройдены, программа работает корректно

## Задание 2: Автоматическое дифференцирование

### 2.1 Простые вычисления с градиентами

**Реализация:**
Реализована функция f(x,y,z) = x² + y² + z² + 2*x*y*z с вычислением градиентов:
- Созданы тензоры с `requires_grad=True`
- Вычислены градиенты через `backward()`
- Проведена аналитическая проверка

**Вывод программы:**
```
f(1.0, 2.0, 3.0) = 26.0
Градиенты:
df/dx = 14.0, df/dy = 10.0, df/dz = 10.0
```

**Аналитическая проверка:**
```
df/dx = 2*1 + 2*2*3 = 14 ✓
df/dy = 2*2 + 2*1*3 = 10 ✓
df/dz = 2*3 + 2*1*2 = 10 ✓
```

**Вывод:** Автоматическое дифференцирование работает корректно, градиенты вычислены верно и совпадают с аналитическими расчетами.

### 2.2 Градиент функции потерь MSE

**Реализация:**
Реализована MSE для линейной модели y_pred = w*x + b:
- Вычислены градиенты по w и b
- Проведена проверка расчетов

**Вывод программы:**
```
MSE = 10.375
Градиенты:
dL/dw = -17.5, dL/db = -5.5
```

**Проверка расчета:**
Для x=[1,2,3,4], y_true=[2,4,6,8], w=0.5, b=1.0:
```
y_pred = [1.5, 2.0, 2.5, 3.0]
dMSE/dw = 2/4 * [(1.5-2)*1 + (2-4)*2 + (2.5-6)*3 + (3-8)*4] = -17.5 ✓
dMSE/db = 2/4 * [(1.5-2) + (2-4) + (2.5-6) + (3-8)] = -5.5 ✓
```

**Вывод:** Градиенты функции потерь MSE вычислены правильно, что подтверждает корректность реализации автоматического дифференцирования для задачи регрессии.

### 2.3 Цепное правило

**Реализация:**
Реализована функция f(x) = sin(x² + 1):
- Градиент вычислен через `backward()`
- Проверен через `torch.autograd.grad()`
- Проведена аналитическая проверка

**Вывод программы:**
```
f(2.0) = -0.9589
Градиенты:
через backward(): 1.1346
через autograd.grad: 1.1346
аналитический: 1.1346
```

**Аналитическая проверка:**
```
df/dx = cos(2² + 1) * 2*2 = cos(5)*4 ≈ 1.1346 ✓
```

**Вывод:** Все три метода вычисления градиента дали одинаковый результат, что подтверждает правильность реализации цепного правила в PyTorch.

### Итог по заданию 2

Автоматическое дифференцирование реализовано успешно:
- Градиенты многопараметрической функции вычислены верно
- Функция потерь MSE и ее градиенты работают корректно
- Цепное правило применено правильно
- Все результаты подтверждены аналитическими расчетами

## Задание 3: Сравнение производительности CPU vs CUDA

### 3.1 Подготовка данных

Созданы три больших тензора:
1. 64x1024x1024 (67,108,864 элементов)
2. 128x512x512 (33,554,432 элементов)
3. 256x256x256 (16,777,216 элементов)

Все тензоры заполнены случайными числами через `torch.rand()`.

### 3.2 Функция измерения времени

Реализована функция `measure_time()`:
- Для CPU: `time.time()`
- Для GPU: `torch.cuda.Event()`
- Учет времени только выполнения операции (без передачи данных)

### 3.3 Сравнение операций (Tesla T4)

**Результаты для матрицы 64x1024x1024:**

| Операция               | CPU (мс) | GPU (мс) | Ускорение |
|------------------------|----------|----------|-----------|
| Матричное умножение    | 2157.5   | 178.6    | 12.1x     |
| Поэлементное сложение  | 169.4    | 42.0     | 4.0x      |
| Поэлементное умножение | 162.9    | 35.5     | 4.6x      |
| Транспонирование       | 0.1      | 0.1      | 1.0x      |
| Сумма всех элементов   | 31.5     | 39.6     | 0.8x      |

**Результаты для матрицы 128x512x512:**

| Операция               | CPU (мс) | GPU (мс) | Ускорение |
|------------------------|----------|----------|-----------|
| Матричное умножение    | 560.0    | 13.2     | 42.3x     |
| Поэлементное сложение  | 82.0     | 1.2      | 69.3x     |
| Поэлементное умножение | 78.4     | 1.2      | 66.0x     |
| Транспонирование       | 0.0      | 0.0      | 1.7x      |
| Сумма всех элементов   | 11.7     | 0.6      | 18.5x     |

**Результаты для матрицы 256x256x256:**

| Операция               | CPU (мс) | GPU (мс) | Ускорение |
|------------------------|----------|----------|-----------|
| Матричное умножение    | 163.0    | 3.6      | 45.0x     |
| Поэлементное сложение  | 38.5     | 0.6      | 59.6x     |
| Поэлементное умножение | 38.1     | 0.6      | 60.3x     |
| Транспонирование       | 0.0      | 0.0      | 1.8x      |
| Сумма всех элементов   | 5.7      | 0.4      | 16.4x     |

### 3.4 Анализ результатов

1. **Наибольшее ускорение** получено для:
   - Поэлементных операций (до 69x)
   - Матричного умножения (до 45x)

2. **Операции с малым ускорением**:
   - Транспонирование (1-1.8x) - ограничено пропускной способностью памяти
   - Сумма элементов (0.8-18.5x) - зависит от размера матрицы

3. **Влияние размера матриц**:
   - Для больших матриц (128x512x512) ускорение максимально
   - Для очень больших (64x1024x1024) возможны ограничения памяти GPU
   - Для меньших (256x256x256) ускорение хорошее, но не максимальное

4. **Передача данных**:
   - Время передачи не учитывалось в тестах
   - Для реальных задач может стать узким местом
   - Рекомендуется минимизировать передачу между CPU и GPU

### Итог по заданию 3

1. GPU обеспечивает значительное ускорение для матричных операций
2. Максимальное ускорение достигается для:
   - Крупных матриц
   - Параллелизуемых операций
3. Эффективность зависит от:
   - Типа операции
   - Размера данных
   - Оптимизации доступа к памяти
4. Для полного использования потенциала GPU необходимо:
   - Правильно подбирать размеры данных
   - Минимизировать передачу между устройствами
   - Использовать оптимизированные операции
